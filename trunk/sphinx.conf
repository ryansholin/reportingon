source questions_question
{
    type                = mysql
    sql_host            = localhost
    sql_user            = root
    sql_pass            = root
    sql_db              = reportingon
    sql_port            = 

    sql_query_pre       =
    sql_query_post      =
    sql_query           = SELECT id, question, slug, author_id, status, created, modified, tags FROM questions_question LIMIT 100
    sql_query_info      = SELECT * FROM `questions_question` WHERE `id` = $id

    # ForeignKey's
    sql_attr_uint       = author_id

    # DateField's and DateTimeField's
    sql_attr_timestamp   = created
    sql_attr_timestamp   = modified
}

index questions_question
{
    source          = questions_question
    path            = /var/data/questions_question
    docinfo         = extern
    morphology      = none
    stopwords       =
    min_word_len    = 2
    charset_type    = sbcs
    min_prefix_len  = 0
    min_infix_len   = 0
}

indexer
{
	# memory limit, in bytes, kiloytes (16384K) or megabytes (256M)
	# optional, default is 32M, max is 2047M, recommended is 256M to 1024M
	mem_limit			= 32M

	# maximum IO calls per second (for I/O throttling)
	# optional, default is 0 (unlimited)
	max_iops			= 60
}

searchd
{
	# IP address to bind on
	# optional, default is 0.0.0.0 (ie. listen on all interfaces)
	#
	# address				= 127.0.0.1
	# address				= 192.168.0.1


	# searchd TCP port number
	# mandatory, default is 3312
	port				= 3312

	# log file, searchd run info is logged here
	# optional, default is 'searchd.log'
	log					= /usr/local/sphinx/log/searchd.log

	# query log file, all search queries are logged here
	# optional, default is empty (do not log queries)
	query_log			= /usr/local/sphinx/log/query.log

	# client read timeout, seconds
	# optional, default is 5
	read_timeout		= 5

	# maximum amount of children to fork (concurrent searches to run)
	# optional, default is 0 (unlimited)
	max_children		= 30

	# PID file, searchd process ID file name
	# mandatory
	pid_file			= /usr/local/sphinx/log/searchd.pid

	# max amount of matches the daemon ever keeps in RAM, per-index
	# WARNING, THERE'S ALSO PER-QUERY LIMIT, SEE SetLimits() API CALL
	# default is 1000 (just like Google)
	max_matches			= 1000

	# seamless rotate, prevents rotate stalls if precaching huge datasets
	# optional, default is 1
	seamless_rotate		= 1

	# whether to forcibly preopen all indexes on startup
	# optional, default is 0 (do not preopen)
	preopen_indexes		= 0

	# whether to unlink .old index copies on succesful rotation.
	# optional, default is 1 (do unlink)
	unlink_old			= 1
}

# --eof--